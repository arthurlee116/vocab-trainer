[
  {
    "id": "openrouter/polaris-alpha",
    "name": "Polaris Alpha",
    "vendor": "openrouter",
    "created": 1762451707,
    "context_length": 256000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "This is a cloaked model provided to the community to gather feedback. A powerful, general-purpose model that excels across real-world tasks, with standout performance in coding, tool calling, and instruction following.",
    "pricing": {
      "prompt": "0",
      "completion": "0",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "amazon/nova-premier-v1",
    "name": "Amazon: Nova Premier 1.0",
    "vendor": "amazon",
    "created": 1761950332,
    "context_length": 1000000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.",
    "pricing": {
      "prompt": "0.0000025",
      "completion": "0.0000125",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000625"
    }
  },
  {
    "id": "perplexity/sonar-pro-search",
    "name": "Perplexity: Sonar Pro Search",
    "vendor": "perplexity",
    "created": 1761854366,
    "context_length": 200000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Exclusively available on the OpenRouter API, Sonar Pro's new Pro Search mode is Perplexity's most advanced agentic search system. It is designed for deeper reasoning and analysis. Pricing is based on tokens plus $18 per thousand requests. This model powers the Pro Search mode on the Perplexity platform.",
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0.018",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "openai/gpt-5-image-mini",
    "name": "OpenAI: GPT-5 Image Mini",
    "vendor": "openai",
    "created": 1760624583,
    "context_length": 400000,
    "input_modalities": [
      "file",
      "image",
      "text"
    ],
    "description": "GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini), with GPT Image 1 Mini for efficient image generation. This natively multimodal model features superior instruction following, text rendering, and detailed image editing with reduced latency and cost. It excels at high-quality visual creation while maintaining strong text understanding, making it ideal for applications that require both efficient image generation and text processing at scale.",
    "pricing": {
      "prompt": "0.0000025",
      "completion": "0.000002",
      "request": "0",
      "image": "0.0000025",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000025"
    }
  },
  {
    "id": "qwen/qwen3-vl-8b-thinking",
    "name": "Qwen: Qwen3 VL 8B Thinking",
    "vendor": "qwen",
    "created": 1760463746,
    "context_length": 256000,
    "input_modalities": [
      "image",
      "text"
    ],
    "description": "Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multimodal model, designed for advanced visual and textual reasoning across complex scenes, documents, and temporal sequences. It integrates enhanced multimodal alignment and long-context processing (native 256K, expandable to 1M tokens) for tasks such as scientific visual analysis, causal inference, and mathematical reasoning over image or video inputs.",
    "pricing": {
      "prompt": "0.00000018",
      "completion": "0.0000021",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen3-vl-8b-instruct",
    "name": "Qwen: Qwen3 VL 8B Instruct",
    "vendor": "qwen",
    "created": 1760463308,
    "context_length": 131072,
    "input_modalities": [
      "image",
      "text"
    ],
    "description": "Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-fidelity understanding and reasoning across text, images, and video. It features improved multimodal fusion with Interleaved-MRoPE for long-horizon temporal reasoning, DeepStack for fine-grained visual-text alignment, and text-timestamp alignment for precise event localization.",
    "pricing": {
      "prompt": "0.00000008",
      "completion": "0.0000005",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0"
    }
  },
  {
    "id": "openai/gpt-5-image",
    "name": "OpenAI: GPT-5 Image",
    "vendor": "openai",
    "created": 1760447986,
    "context_length": 400000,
    "input_modalities": [
      "image",
      "text",
      "file"
    ],
    "description": "[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's most advanced language model with state-of-the-art image generation capabilities. It offers major improvements in reasoning, code quality, and user experience while incorporating GPT Image 1's superior instruction following, text rendering, and detailed image editing.",
    "pricing": {
      "prompt": "0.00001",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00001",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000125"
    }
  },
  {
    "id": "google/gemini-2.5-flash-image",
    "name": "Google: Gemini 2.5 Flash Image (Nano Banana)",
    "vendor": "google",
    "created": 1759870431,
    "context_length": 32768,
    "input_modalities": [
      "image",
      "text"
    ],
    "description": "Gemini 2.5 Flash Image, a.k.a. \"Nano Banana,\" is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)",
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen3-vl-30b-a3b-thinking",
    "name": "Qwen: Qwen3 VL 30B A3B Thinking",
    "vendor": "qwen",
    "created": 1759794479,
    "context_length": 131072,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen3-VL-30B-A3B-Thinking is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Thinking variant enhances reasoning in STEM, math, and complex tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.",
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.000001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0"
    }
  },
  {
    "id": "qwen/qwen3-vl-30b-a3b-instruct",
    "name": "Qwen: Qwen3 VL 30B A3B Instruct",
    "vendor": "qwen",
    "created": 1759794476,
    "context_length": 262144,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen3-VL-30B-A3B-Instruct is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Instruct variant optimizes instruction-following for general multimodal tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.",
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.0000006",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "openai/gpt-5-pro",
    "name": "OpenAI: GPT-5 Pro",
    "vendor": "openai",
    "created": 1759776663,
    "context_length": 400000,
    "input_modalities": [
      "image",
      "text",
      "file"
    ],
    "description": "GPT-5 Pro is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
    "pricing": {
      "prompt": "0.000015",
      "completion": "0.00012",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "anthropic/claude-sonnet-4.5",
    "name": "Anthropic: Claude Sonnet 4.5",
    "vendor": "anthropic",
    "created": 1759161676,
    "context_length": 1000000,
    "input_modalities": [
      "text",
      "image",
      "file"
    ],
    "description": "Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.",
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000003",
      "input_cache_write": "0.00000375"
    }
  },
  {
    "id": "google/gemini-2.5-flash-preview-09-2025",
    "name": "Google: Gemini 2.5 Flash Preview 09-2025",
    "vendor": "google",
    "created": 1758820178,
    "context_length": 1048576,
    "input_modalities": [
      "image",
      "file",
      "text",
      "audio",
      "video"
    ],
    "description": "Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. ",
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "audio": "0.000001",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000075",
      "input_cache_write": "0.0000003833"
    }
  },
  {
    "id": "google/gemini-2.5-flash-lite-preview-09-2025",
    "name": "Google: Gemini 2.5 Flash Lite Preview 09-2025",
    "vendor": "google",
    "created": 1758819686,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image",
      "file",
      "audio",
      "video"
    ],
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen3-vl-235b-a22b-thinking",
    "name": "Qwen: Qwen3 VL 235B A22B Thinking",
    "vendor": "qwen",
    "created": 1758668690,
    "context_length": 262144,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen3-VL-235B-A22B Thinking is a multimodal model that unifies strong text generation with visual understanding across images and video. The Thinking model is optimized for multimodal reasoning in STEM and math. The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.",
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000012",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen3-vl-235b-a22b-instruct",
    "name": "Qwen: Qwen3 VL 235B A22B Instruct",
    "vendor": "qwen",
    "created": 1758668687,
    "context_length": 262144,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.",
    "pricing": {
      "prompt": "0.00000022",
      "completion": "0.00000088",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "openai/gpt-5-codex",
    "name": "OpenAI: GPT-5 Codex",
    "vendor": "openai",
    "created": 1758643403,
    "context_length": 400000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)",
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    }
  },
  {
    "id": "google/gemini-2.5-flash-image-preview",
    "name": "Google: Gemini 2.5 Flash Image Preview (Nano Banana)",
    "vendor": "google",
    "created": 1756218977,
    "context_length": 32768,
    "input_modalities": [
      "image",
      "text"
    ],
    "description": "Gemini 2.5 Flash Image Preview, a.k.a. \"Nano Banana,\" is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations.",
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "openai/gpt-5-chat",
    "name": "OpenAI: GPT-5 Chat",
    "vendor": "openai",
    "created": 1754587837,
    "context_length": 128000,
    "input_modalities": [
      "file",
      "image",
      "text"
    ],
    "description": "GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.",
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    }
  },
  {
    "id": "openai/gpt-5",
    "name": "OpenAI: GPT-5",
    "vendor": "openai",
    "created": 1754587413,
    "context_length": 400000,
    "input_modalities": [
      "text",
      "image",
      "file"
    ],
    "description": "GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    }
  },
  {
    "id": "openai/gpt-5-mini",
    "name": "OpenAI: GPT-5 Mini",
    "vendor": "openai",
    "created": 1754587407,
    "context_length": 400000,
    "input_modalities": [
      "text",
      "image",
      "file"
    ],
    "description": "GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.",
    "pricing": {
      "prompt": "0.00000025",
      "completion": "0.000002",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025"
    }
  },
  {
    "id": "openai/gpt-5-nano",
    "name": "OpenAI: GPT-5 Nano",
    "vendor": "openai",
    "created": 1754587402,
    "context_length": 400000,
    "input_modalities": [
      "text",
      "image",
      "file"
    ],
    "description": "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.",
    "pricing": {
      "prompt": "0.00000005",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000005"
    }
  },
  {
    "id": "google/gemini-2.5-flash-lite",
    "name": "Google: Gemini 2.5 Flash Lite",
    "vendor": "google",
    "created": 1753200276,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image",
      "file",
      "audio",
      "video"
    ],
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000001",
      "input_cache_write": "0.0000001833"
    }
  },
  {
    "id": "google/gemini-2.5-flash-lite-preview-06-17",
    "name": "Google: Gemini 2.5 Flash Lite Preview 06-17",
    "vendor": "google",
    "created": 1750173831,
    "context_length": 1048576,
    "input_modalities": [
      "file",
      "image",
      "text",
      "audio"
    ],
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "audio": "0.0000003",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025",
      "input_cache_write": "0.0000001833"
    }
  },
  {
    "id": "google/gemini-2.5-flash",
    "name": "Google: Gemini 2.5 Flash",
    "vendor": "google",
    "created": 1750172488,
    "context_length": 1048576,
    "input_modalities": [
      "file",
      "image",
      "text",
      "audio",
      "video"
    ],
    "description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. ",
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000003",
      "input_cache_write": "0.0000003833"
    }
  },
  {
    "id": "google/gemini-2.5-pro",
    "name": "Google: Gemini 2.5 Pro",
    "vendor": "google",
    "created": 1750169544,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image",
      "file",
      "audio",
      "video"
    ],
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00516",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125",
      "input_cache_write": "0.000001625"
    }
  },
  {
    "id": "google/gemini-2.5-pro-preview",
    "name": "Google: Gemini 2.5 Pro Preview 06-05",
    "vendor": "google",
    "created": 1749137257,
    "context_length": 1048576,
    "input_modalities": [
      "file",
      "image",
      "text",
      "audio"
    ],
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00516",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000031",
      "input_cache_write": "0.000001625"
    }
  },
  {
    "id": "google/gemini-2.5-pro-preview-05-06",
    "name": "Google: Gemini 2.5 Pro Preview 05-06",
    "vendor": "google",
    "created": 1746578513,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image",
      "file",
      "audio",
      "video"
    ],
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00516",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000031",
      "input_cache_write": "0.000001625"
    }
  },
  {
    "id": "meta-llama/llama-guard-4-12b",
    "name": "Meta: Llama Guard 4 12B",
    "vendor": "meta-llama",
    "created": 1745975193,
    "context_length": 163840,
    "input_modalities": [
      "image",
      "text"
    ],
    "description": "Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM—generating text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.",
    "pricing": {
      "prompt": "0.00000018",
      "completion": "0.00000018",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "meta-llama/llama-4-maverick:free",
    "name": "Meta: Llama 4 Maverick (free)",
    "vendor": "meta-llama",
    "created": 1743881822,
    "context_length": 128000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.",
    "pricing": {
      "prompt": "0",
      "completion": "0",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "meta-llama/llama-4-maverick",
    "name": "Meta: Llama 4 Maverick",
    "vendor": "meta-llama",
    "created": 1743881822,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.",
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.0000006",
      "request": "0",
      "image": "0.0006684",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "meta-llama/llama-4-scout:free",
    "name": "Meta: Llama 4 Scout (free)",
    "vendor": "meta-llama",
    "created": 1743881519,
    "context_length": 128000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.",
    "pricing": {
      "prompt": "0",
      "completion": "0",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "meta-llama/llama-4-scout",
    "name": "Meta: Llama 4 Scout",
    "vendor": "meta-llama",
    "created": 1743881519,
    "context_length": 327680,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.",
    "pricing": {
      "prompt": "0.00000008",
      "completion": "0.0000003",
      "request": "0",
      "image": "0.0003342",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen2.5-vl-32b-instruct:free",
    "name": "Qwen: Qwen2.5 VL 32B Instruct (free)",
    "vendor": "qwen",
    "created": 1742839838,
    "context_length": 16384,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.",
    "pricing": {
      "prompt": "0",
      "completion": "0",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen2.5-vl-32b-instruct",
    "name": "Qwen: Qwen2.5 VL 32B Instruct",
    "vendor": "qwen",
    "created": 1742839838,
    "context_length": 16384,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.",
    "pricing": {
      "prompt": "0.00000005",
      "completion": "0.00000022",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "perplexity/sonar-reasoning-pro",
    "name": "Perplexity: Sonar Reasoning Pro",
    "vendor": "perplexity",
    "created": 1741313308,
    "context_length": 128000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)",
    "pricing": {
      "prompt": "0.000002",
      "completion": "0.000008",
      "request": "0",
      "image": "0",
      "web_search": "0.005",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "perplexity/sonar-pro",
    "name": "Perplexity: Sonar Pro",
    "vendor": "perplexity",
    "created": 1741312423,
    "context_length": 200000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)",
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0",
      "web_search": "0.005",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "google/gemini-2.0-flash-lite-001",
    "name": "Google: Gemini 2.0 Flash Lite",
    "vendor": "google",
    "created": 1740506212,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image",
      "file",
      "audio"
    ],
    "description": "Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.",
    "pricing": {
      "prompt": "0.000000075",
      "completion": "0.0000003",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "google/gemini-2.0-flash-001",
    "name": "Google: Gemini 2.0 Flash",
    "vendor": "google",
    "created": 1738769413,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image",
      "file",
      "audio"
    ],
    "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0.0000258",
      "audio": "0.0000007",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025",
      "input_cache_write": "0.0000001833"
    }
  },
  {
    "id": "qwen/qwen-vl-plus",
    "name": "Qwen: Qwen VL Plus",
    "vendor": "qwen",
    "created": 1738731255,
    "context_length": 7500,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.",
    "pricing": {
      "prompt": "0.00000021",
      "completion": "0.00000063",
      "request": "0",
      "image": "0.0002688",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen-vl-max",
    "name": "Qwen: Qwen VL Max",
    "vendor": "qwen",
    "created": 1738434304,
    "context_length": 131072,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering optimal performance for a broader spectrum of complex tasks.",
    "pricing": {
      "prompt": "0.0000008",
      "completion": "0.0000032",
      "request": "0",
      "image": "0.001024",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen2.5-vl-72b-instruct",
    "name": "Qwen: Qwen2.5 VL 72B Instruct",
    "vendor": "qwen",
    "created": 1738410311,
    "context_length": 32768,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It is also highly capable of analyzing texts, charts, icons, graphics, and layouts within images.",
    "pricing": {
      "prompt": "0.00000008",
      "completion": "0.00000033",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "perplexity/sonar",
    "name": "Perplexity: Sonar",
    "vendor": "perplexity",
    "created": 1738013808,
    "context_length": 127072,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Sonar is lightweight, affordable, fast, and simple to use — now featuring citations and the ability to customize sources. It is designed for companies seeking to integrate lightweight question-and-answer features optimized for speed.",
    "pricing": {
      "prompt": "0.000001",
      "completion": "0.000001",
      "request": "0.005",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "minimax/minimax-01",
    "name": "MiniMax: MiniMax-01",
    "vendor": "minimax",
    "created": 1736915462,
    "context_length": 1000192,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding. It has 456 billion parameters, with 45.9 billion parameters activated per inference, and can handle a context of up to 4 million tokens.",
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000011",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "google/gemini-2.0-flash-exp:free",
    "name": "Google: Gemini 2.0 Flash Experimental (free)",
    "vendor": "google",
    "created": 1733937523,
    "context_length": 1048576,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
    "pricing": {
      "prompt": "0",
      "completion": "0",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "amazon/nova-lite-v1",
    "name": "Amazon: Nova Lite 1.0",
    "vendor": "amazon",
    "created": 1733437363,
    "context_length": 300000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy.",
    "pricing": {
      "prompt": "0.00000006",
      "completion": "0.00000024",
      "request": "0",
      "image": "0.00009",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "amazon/nova-pro-v1",
    "name": "Amazon: Nova Pro 1.0",
    "vendor": "amazon",
    "created": 1733436303,
    "context_length": 300000,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).",
    "pricing": {
      "prompt": "0.0000008",
      "completion": "0.0000032",
      "request": "0",
      "image": "0.0012",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "meta-llama/llama-3.2-90b-vision-instruct",
    "name": "Meta: Llama 3.2 90B Vision Instruct",
    "vendor": "meta-llama",
    "created": 1727222400,
    "context_length": 32768,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "The Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.",
    "pricing": {
      "prompt": "0.00000035",
      "completion": "0.0000004",
      "request": "0",
      "image": "0.0005058",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "meta-llama/llama-3.2-11b-vision-instruct",
    "name": "Meta: Llama 3.2 11B Vision Instruct",
    "vendor": "meta-llama",
    "created": 1727222400,
    "context_length": 131072,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Llama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.",
    "pricing": {
      "prompt": "0.000000049",
      "completion": "0.000000049",
      "request": "0",
      "image": "0.00007948",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  },
  {
    "id": "qwen/qwen-2.5-vl-7b-instruct",
    "name": "Qwen: Qwen2.5-VL 7B Instruct",
    "vendor": "qwen",
    "created": 1724803200,
    "context_length": 32768,
    "input_modalities": [
      "text",
      "image"
    ],
    "description": "Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:",
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000002",
      "request": "0",
      "image": "0.0001445",
      "web_search": "0",
      "internal_reasoning": "0"
    }
  }
]